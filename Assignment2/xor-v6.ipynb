{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactoring tips : https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "# Module forward hook : https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/\n",
    "# Module, Sequential, and organizing better : https://github.com/FrancescoSaverioZuppichini/Pytorch-how-and-when-to-use-Module-Sequential-ModuleList-and-ModuleDict/blob/master/README.md\n",
    "# writing your own optimizer http://mcneela.github.io/machine_learning/2019/09/03/Writing-Your-Own-Optimizers-In-Pytorch.html\n",
    "# torch tensor quick start operatiosn https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/\n",
    "# SGD optimizer codein pytorch https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD.step\n",
    "# Understanding torch nn Parameter https://stackoverflow.com/questions/50935345/understanding-torch-nn-parameter\n",
    "\n",
    "# Not used:\n",
    "# https://medium.com/pytorch/accelerate-your-hyperparameter-optimization-with-pytorchs-ecosystem-tools-bc17001b9a49\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XORDataset(Dataset):\n",
    "    \"\"\"XOR dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, num_samples = 200):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples: number of samples to generate\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.X = np.random.randint(0,high=2, size=num_samples*2).reshape((num_samples, 2)).astype(np.float32)\n",
    "        self.y = np.logical_xor(self.X[:,0], self.X[:,1]).reshape((num_samples,1)).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        #sample = {'X': self.X[idx], 'y': self.y[idx]}\n",
    "        sample = (self.X[idx], self.y[idx])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders, get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, num_samples):\n",
    "\n",
    "    datasets = {\n",
    "        'train' : XORDataset(num_samples),\n",
    "        'valid' : XORDataset(50),\n",
    "    }\n",
    "    loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(datasets['train'],  \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1),\n",
    "    'valid' : torch.utils.data.DataLoader(datasets['valid'], \n",
    "                                          batch_size=10, \n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1),\n",
    "    }\n",
    "    return datasets, loaders\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net model\n",
    "## class XORnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XORnet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(XORnet, self).__init__()\n",
    "\n",
    "        self.fc1=nn.Linear(2, 2, bias=True)\n",
    "        self.sig1=nn.Sigmoid()\n",
    "        self.fc2=nn.Linear(2, 1, bias=True)\n",
    "        self.sig2=nn.Sigmoid()\n",
    "        \n",
    "        self.activations=[0 for i in range(3)] # 3 \"layers\" : input/hidden/output\n",
    "\n",
    "        # custom weight initialization\n",
    "\n",
    "        for layer in [self.fc1, self.fc2]:\n",
    "            nn.init.normal_(layer.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        \n",
    "        # not sure if necessary ??\n",
    "        torch.nn.init.constant_(self.fc1.bias.data,1)\n",
    "        torch.nn.init.constant_(self.fc2.bias.data,1)\n",
    "\n",
    "        x2 = self.fc1(x)\n",
    "        x2 = self.sig1(x2)\n",
    "        x3 = self.fc2(x2)\n",
    "        x3 = self.sig2(x3)\n",
    "        self.store_activations(x, x2, x3)\n",
    "        return x3\n",
    "\n",
    "    def store_activations(self, x, x2, x3):\n",
    "        self.activations[0] = x\n",
    "        self.activations[1] = x2\n",
    "        self.activations[2] = x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To make comparisons easier, we use the\\nsame learning rate η for FF+FB and FF, determined by grid search. \\nWe average 10 runs, taking g = 0.7 and η = 0.025 for both networks. The number of\\nepochs is capped to 300 epochs. With these constraints, η is optimized (by grid search) for the smallest\\nnumber of epochs of the two networks that reached 100% training accuracy in 10 runs.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to do : determine learning rate by grid search\n",
    "'''To make comparisons easier, we use the\n",
    "same learning rate η for FF+FB and FF, determined by grid search. \n",
    "We average 10 runs, taking g = 0.7 and η = 0.025 for both networks. The number of\n",
    "epochs is capped to 300 epochs. With these constraints, η is optimized (by grid search) for the smallest\n",
    "number of epochs of the two networks that reached 100% training accuracy in 10 runs.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class statsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "import pathlib\n",
    "\n",
    "class statsLogger():\n",
    "    def __init__(self, learner=None):\n",
    "        self.accuracies=[]   # accuracy for each epoch\n",
    "        self.losses=[]\n",
    "        self.epochs=0\n",
    "        self.minibatches=0\n",
    "        self.samples=0\n",
    "        self.all_minibatches_accuracies=[]  # accuracies for all minibatches in all epochs\n",
    "        self.all_minibatches_losses=[]  # accuracies for all minibatches in all epochs\n",
    "        self.learner = learner\n",
    "        self.bs=None\n",
    "        self.run_name=None\n",
    "\n",
    "    def getRunName(self):\n",
    "        if self.run_name == None:\n",
    "            today = date.today()\n",
    "            now=datetime.now().strftime(\"%H%M%S\")\n",
    "            run_name=f'{today}-{now}-'\n",
    "\n",
    "            learner=self.learner\n",
    "\n",
    "            if learner is not None:\n",
    "                run_name+=f'{learner.model.__class__.__name__}-{learner.opt.__class__.__name__}-{learner.lr}-bs{self.bs}'\n",
    "            self.run_name = run_name\n",
    "\n",
    "        return self.run_name\n",
    "        \n",
    "\n",
    "    def startEpoch(self):\n",
    "        self.epoch_total_samples=0\n",
    "        self.epoch_total_minibatches=0\n",
    "        self.epoch_correct_samples=0\n",
    "        self.minibatch_losses=[]  # losses for all minibatches in the epoch\n",
    "        \n",
    "\n",
    "    def getMinibatchStats(self, pred, yb, loss, pref_fn=lambda pred : (pred>0.5).float() ):\n",
    "\n",
    "        #Accuracy\n",
    "        predicted = pref_fn(pred)\n",
    "        minibatch_num_corrects = (predicted == yb).float().sum()\n",
    "        minibatch_total = pred.shape[0]\n",
    "        if self.bs == None:\n",
    "            self.bs = minibatch_total\n",
    "        self.epoch_total_samples += minibatch_total\n",
    "        self.epoch_correct_samples += minibatch_num_corrects.item()\n",
    "        self.minibatch_losses.append(loss.item())\n",
    "        self.all_minibatches_losses.append(loss.item())\n",
    "        self.all_minibatches_accuracies.append(100 *  minibatch_num_corrects.item() / minibatch_total)\n",
    "        self.minibatches+=1\n",
    "        self.epoch_total_minibatches+=1\n",
    "\n",
    "    def endEpoch(self):\n",
    "        self.losses.append(np.array(self.minibatch_losses).mean())\n",
    "        epoch_accuracy = 100 * self.epoch_correct_samples / self.epoch_total_samples\n",
    "        self.accuracies.append(epoch_accuracy)\n",
    "        self.epochs+=1\n",
    "        self.samples+=self.epoch_total_samples\n",
    "        return epoch_accuracy\n",
    "\n",
    "    def save(self, train_or_val: str, target_accuracy):\n",
    "\n",
    "        learner = self.learner\n",
    "        if learner is not None:\n",
    "            filename=f'{train_or_val}-{learner.model.__class__.__name__}-{learner.opt.__class__.__name__}-lr{learner.lr}-bs{self.bs}.csv'\n",
    "        else:\n",
    "            filename=f'{train_or_val}-bs{self.bs}.csv'\n",
    "          \n",
    "        run_name = self.getRunName()\n",
    "        epoch_with_target_accuracy_or_last_epoch = next((i+1 for i, acc in enumerate(self.accuracies) if acc >= 100), len(self.accuracies))\n",
    "        print(f'Target accuracy of {target_accuracy} achieved at epoch {epoch_with_target_accuracy_or_last_epoch} : \\\n",
    "             loss {self.losses[epoch_with_target_accuracy_or_last_epoch-1]}, accuracy {self.accuracies[epoch_with_target_accuracy_or_last_epoch-1]}')\n",
    "        info = {\n",
    "            'run_name': [run_name],\n",
    "            'target_epoch_accuracy': [target_accuracy],\n",
    "            'epoch': [epoch_with_target_accuracy_or_last_epoch], \n",
    "            'epoch_loss': [self.losses[epoch_with_target_accuracy_or_last_epoch-1]],\n",
    "            'epoch_accuracy': [self.accuracies[epoch_with_target_accuracy_or_last_epoch-1]]\n",
    "        }\n",
    "\n",
    "        file = pathlib.Path(filename)\n",
    "        print(f'Saving epoch to reach target accuracy to {filename}')\n",
    "\n",
    "        if file.exists ():\n",
    "\n",
    "            df = pd.read_csv(filename)\n",
    "            df2 = pd.DataFrame(info)\n",
    "            result = pd.concat([df, df2])\n",
    "            result.to_csv(filename, sep=',', index=None)\n",
    "        else:\n",
    "\n",
    "            df = pd.DataFrame(info)\n",
    "            df.to_csv(filename, sep=',', index=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory measures\n",
    "\n",
    "## class ActivationsCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "\n",
    "class activationsCollector():\n",
    "    def __init__(self, g):\n",
    "        self.g = g\n",
    "        # 5 because 2 input, 2 hidden layer neurons ad 1 output in perceptron ==> 5 activations to store\n",
    "        self.neurons={}\n",
    "        self.binNeurons={}\n",
    "        self.targets=[]\n",
    "        self.num_layers=0\n",
    "        self.num_neurons_per_layer=[]\n",
    "        self.bs=0\n",
    "\n",
    "    def collect(self, model, yb):\n",
    "\n",
    "        self.bs=yb.size()[0]         # the first dim of the activation tensor or target tensor is the number of samples in minibatch\n",
    "        bs=self.bs\n",
    "\n",
    "        # storing the targets\n",
    "        self.targets.extend([yb[i].item() for i in range(bs)])\n",
    "\n",
    "        activations=model.activations\n",
    "\n",
    "        # Layer details initializations on first time use\n",
    "        if self.num_layers==0:\n",
    "            self.num_layers=len(activations)\n",
    "            for l in range(self.num_layers):\n",
    "                self.num_neurons_per_layer.append(activations[l].size()[1])  # the second dimensions of the activations in a layer is the number of neurons in that layer\n",
    "\n",
    "        # collecting activations browing through the whole net model    \n",
    "        for l in range(self.num_layers):\n",
    "            # self.num_neurons_per_layer.append(activations[l].size()[1])\n",
    "\n",
    "            #for neuron in range(activations[l].size()[1]): \n",
    "            for neuron in range(self.num_neurons_per_layer[l]): \n",
    "\n",
    "                neuron_name=f'{l}-{neuron}'\n",
    "                if (neuron_name not in self.neurons):\n",
    "                    self.neurons[neuron_name]=[]\n",
    "                    self.binNeurons[neuron_name]=[]\n",
    "\n",
    "                self.neurons[neuron_name].extend([activations[l][i, neuron].item() for i in range(bs)])\n",
    "                self.binNeurons[neuron_name].extend([int(activations[l][i, neuron].item() > self.g) for i in range(bs)])\n",
    "\n",
    "                #if neuron_name=='1-1':\n",
    "                #print(f'Neuron {neuron_name}:')\n",
    "                #print('activation:', [activations[l][i, neuron].item() for i in range(bs)])\n",
    "                #print('discretized:', [int(activations[l][i, neuron].item() > self.g) for i in range(bs)])\n",
    "\n",
    "    def save(self, run_name):\n",
    "\n",
    "        filename=f'{run_name}-'\n",
    "\n",
    "        name_neurons = filename + 'neurons-act.csv'\n",
    "        name_bin_neurons = filename + f'thres{self.g}-bin-neurons-act.csv'\n",
    "        df_neuron = pd.DataFrame(self.neurons)\n",
    "        df_bin_neurons = pd.DataFrame(self.binNeurons)\n",
    "\n",
    "        df_neuron.to_csv(name_neurons, sep=',', index=False)\n",
    "        df_bin_neurons.to_csv(name_bin_neurons, sep=',', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## te calculator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jpype import startJVM, getDefaultJVMPath, JArray, JDouble, JInt, JPackage, shutdownJVM, isJVMStarted\n",
    "\n",
    "javaIntList = lambda l : JArray(JInt, 1)(l)\n",
    "javaDoubleList = lambda l : JArray(JDouble, 1)(l)\n",
    "# import sys \n",
    "# Our python data file readers are a bit of a hack, python users will do better on this:\n",
    "# sys.path.append(\"/home/jovyan/notebooks/jidt/demos/python\")\n",
    "\n",
    "# do we make the assumption that the process is stationary when calculating TE?\n",
    "# what is the history length that we can consider ?\n",
    "\n",
    "class teCalculator():\n",
    "    def __init__(self, model, past, bs, calculator_params):\n",
    "        self.past=past\n",
    "        self.bs=bs\n",
    "        self.model=model\n",
    "        self.calculator_params=calculator_params\n",
    "\n",
    "        self.__startJVM()\n",
    "\n",
    "        self.teHistory={}\n",
    "        self.teHistoryLocal={}\n",
    "        self.miHistory={}\n",
    "        self.miHistoryLocal={}\n",
    "        self.tekHistory={}\n",
    "        self.tekHistoryLocal={}\n",
    "        self.mikHistory={}\n",
    "        self.mikHistoryLocal={}\n",
    "\n",
    "    def save(self, run_name):\n",
    "\n",
    "        filename=f'{run_name}-'\n",
    "\n",
    "        if len(self.teHistory['0-0-1-0']) > 1:\n",
    "            name_te = filename + 'interneuron-avgTE.csv'\n",
    "            df_te = pd.DataFrame(self.teHistory)\n",
    "            df_te.to_csv(name_te, sep=',', index=False)\n",
    "\n",
    "        if len(self.tekHistory['0-0-1-0']) > 1:\n",
    "            name_tek = filename + 'interneuron-avgTEK.csv'\n",
    "            df_tek = pd.DataFrame(self.tekHistory)\n",
    "            df_tek.to_csv(name_tek, sep=',', index=False)\n",
    "\n",
    "        if len(self.miHistory['0-0-1-0']) > 1:\n",
    "            name_mi = filename + 'interlayer-avgMI.csv'\n",
    "            df_mi = pd.DataFrame(self.miHistory)\n",
    "            df_mi.to_csv(name_mi, sep=',', index=False)\n",
    "\n",
    "        if len(self.mikHistory['0-0-1-0']) > 1:\n",
    "            name_mik = filename + 'interlayer-avgMI.csv'\n",
    "            df_mik = pd.DataFrame(self.mikHistory)\n",
    "            df_mik.to_csv(name_mik, sep=',', index=False)      \n",
    "        \n",
    "\n",
    "    def __startJVM(self):\n",
    "        # Add JIDT jar library to the path\n",
    "        jarLocation = \"/home/jovyan/notebooks/jidt/infodynamics.jar\"\n",
    "        # Start the JVM (add the \"-Xmx\" option with say 1024M if you get crashes due to not enough memory space)\n",
    "        if (not isJVMStarted()):\n",
    "            startJVM(getDefaultJVMPath(), \"-ea\", \"-Djava.class.path=\" + jarLocation)\n",
    "    \n",
    "    def __setupTECalculator(self, target_history, source_history):\n",
    "        calcClass = JPackage(\"infodynamics.measures.discrete\").TransferEntropyCalculatorDiscrete\n",
    "        base=2\n",
    "        k_history=target_history\n",
    "        k_tau=1\n",
    "        i_history=source_history\n",
    "        i_tau=1\n",
    "        delay=0\n",
    "        calc = calcClass(base, k_history, k_tau, i_history, i_tau, delay)\n",
    "        calc.initialise()\n",
    "\n",
    "        return calc\n",
    "\n",
    "    def __setupTEKraskovCalculator(self):\n",
    "        calcClass = JPackage(\"infodynamics.measures.continuous.kraskov\").TransferEntropyCalculatorKraskov\n",
    "        calc = calcClass()\n",
    "        # 2. Set any properties to non-default values:\n",
    "        calc.setProperty(\"DELAY\", \"0\")\n",
    "        # 3. Initialise the calculator for (re-)use:\n",
    "        calc.initialise()\n",
    "\n",
    "        return calc\n",
    "\n",
    "    def __setupMICalculator(self):\n",
    "        \n",
    "        # 1. Construct the calculator:\n",
    "        calcClass = JPackage(\"infodynamics.measures.discrete\").MutualInformationCalculatorDiscrete\n",
    "        base=2\n",
    "        time_difference=0\n",
    "        calc = calcClass(base, base, time_difference)\n",
    "        # 2. No other properties to set for discrete calculators.\n",
    "        # 3. Initialise the calculator for (re-)use:\n",
    "        calc.initialise()\n",
    "        return calc\n",
    "\n",
    "    def __setupMIKraskovCalculator(self):\n",
    "        # 1. Construct the calculator:\n",
    "        calcClass = JPackage(\"infodynamics.measures.continuous.kraskov\").MutualInfoCalculatorMultiVariateKraskov1\n",
    "        calc = calcClass()\n",
    "        # 2. Set any properties to non-default values:\n",
    "        # No properties were set to non-default values\n",
    "        # 3. Initialise the calculator for (re-)use:\n",
    "        calc.initialise()\n",
    "        return calc\n",
    "\n",
    "    def __extractTimeSeries(self, collector, neuron_i, neuron_j, past=0, binarized=True):\n",
    "\n",
    "        source_layer, i = neuron_i\n",
    "        target_layer, j = neuron_j\n",
    "\n",
    "        if binarized==True:\n",
    "            neurons = collector.binNeurons\n",
    "        else:\n",
    "            neurons = collector.neurons\n",
    "\n",
    "        if past == 0:\n",
    "            # take all we got to calculate time series\n",
    "            source = neurons[f'{source_layer}-{i}']\n",
    "            target = neurons[f'{target_layer}-{j}']\n",
    "        else:\n",
    "            # consider only the nearby \"past\"\n",
    "            source = neurons[f'{source_layer}-{i}'][-past:]\n",
    "            target = neurons[f'{target_layer}-{j}'][-past:]\n",
    "        #print(f'{neuron_i} source activations: {source} - {neuron_j} : target activations {target}, past = {past}')\n",
    "\n",
    "        return source, target\n",
    "\n",
    "    def calculateMeasureBetweenNeurons(self, collector, neuron_i, neuron_j, past, calc_setup_fn, calc_name='discrete', p_value=100):\n",
    "\n",
    "        # Setup for Kraskov TE - Need to use the non binarized neuron output and also not calculate bias,\n",
    "        # because Kraskov is already bias corrected\n",
    "\n",
    "        bias = True\n",
    "        binarized = True\n",
    "        if calc_name == 'kraskov':\n",
    "            bias=False\n",
    "            binarized=False\n",
    "        \n",
    "        # Extracting time series out of neurons activations stored in collector\n",
    "\n",
    "        source, target = self.__extractTimeSeries(collector, neuron_i, neuron_j, self.past, binarized)\n",
    "        number_of_samples=len(source)\n",
    "\n",
    "        if binarized == True:\n",
    "            source = javaIntList(source)\n",
    "            target = javaIntList(target)\n",
    "        else:\n",
    "            source = javaDoubleList(source)\n",
    "            target = javaDoubleList(target)\n",
    "\n",
    "        # set up calculator\n",
    "        calc=calc_setup_fn()\n",
    "\n",
    "        # 4. Supply the sample data:\n",
    "    \n",
    "        if calc_name == 'kraskov':\n",
    "\n",
    "            if number_of_samples >= past:\n",
    "                calc.setObservations(source, target)\n",
    "                result=float(calc.computeAverageLocalOfObservations())\n",
    "                if p_value < 100:\n",
    "                    measDist = calc.computeSignificance(100)\n",
    "                    if measDist.pValue > p_value:\n",
    "                        result=0\n",
    "            else:\n",
    "                result=0\n",
    "                \n",
    "            local_results=[]\n",
    "\n",
    "        else:\n",
    "            if number_of_samples >= past:\n",
    "                calc.addObservations(source, target)\n",
    "                result=float(calc.computeAverageLocalOfObservations())\n",
    "                measDist = calc.computeSignificance()\n",
    "                bias = measDist.getMeanOfDistribution()\n",
    "                result-=float(bias)\n",
    "                if p_value < 100:\n",
    "                    measDist = calc.computeSignificance(100)\n",
    "                    if float(measDist.pValue) > p_value:\n",
    "                        result=0\n",
    "            else:\n",
    "                result=0\n",
    "            \n",
    "            # to do if I use local_results - add bias\n",
    "            #TEMP to save time - local_results = calc.computeLocalFromPreviousObservations(source, target)\n",
    "            local_results=[]\n",
    "            \n",
    "        return result, local_results\n",
    "\n",
    "    def __getNeuronPairName(self, neuron_i, neuron_j):\n",
    "        source_layer, i = neuron_i\n",
    "        target_layer, j = neuron_j\n",
    "        pair_name=f'{source_layer}-{i}-{target_layer}-{j}'\n",
    "        return pair_name\n",
    "\n",
    "    def __saveMeasure(self, pair_name, history, historyLocals, result, resultLocals):\n",
    "\n",
    "        if (pair_name not in history):\n",
    "            history[pair_name]=[]\n",
    "            historyLocals[pair_name]=[]\n",
    "\n",
    "        history[pair_name].append(result)\n",
    "        historyLocals[pair_name].extend(resultLocals)\n",
    "\n",
    "    def getLastValueInHistory(self, history, neuron_i, neuron_j):\n",
    "        pair_name = self.__getNeuronPairName(neuron_i, neuron_j)\n",
    "        if pair_name in history and len(history[pair_name]) > 0:\n",
    "            return history[pair_name][-1:][0]\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def getMeasuresForLayer(self, collector, source_layer, target_layer, measures={'te':True, 'mi':True, 'tek':True, 'mik': True}):\n",
    "        \n",
    "        num_sources=collector.num_neurons_per_layer[source_layer]\n",
    "        num_targets=collector.num_neurons_per_layer[target_layer]\n",
    "\n",
    "        for i in range(num_sources):\n",
    "            for j in range(num_targets):\n",
    "\n",
    "                neuron_i = (source_layer, i)\n",
    "                neuron_j = (target_layer, j)\n",
    "                pair_name = self.__getNeuronPairName(neuron_i, neuron_j)\n",
    "\n",
    "                if measures['te']==True:\n",
    "\n",
    "                    # Calculate transfer entropy between neurons\n",
    "\n",
    "                    calc_setup_fn = lambda : self.__setupTECalculator(**self.calculator_params['te'])  \n",
    "                    resultTE, local_resultsTE = self.calculateMeasureBetweenNeurons(collector, neuron_i, neuron_j, self.past, calc_setup_fn, p_value=100)\n",
    "\n",
    "                    # Save average value and local values for each batch size\n",
    "\n",
    "                    self.__saveMeasure(pair_name, self.teHistory, self.teHistoryLocal, resultTE, local_resultsTE[-self.bs:])\n",
    "\n",
    "                if measures['mi']==True:\n",
    "                    # Calculate MI between neurons\n",
    "\n",
    "                    calc_setup_fn = lambda : self.__setupMICalculator()  \n",
    "                    resultMI, local_resultsMI = self.calculateMeasureBetweenNeurons(collector, neuron_i, neuron_j, self.past, calc_setup_fn, p_value=100)\n",
    "\n",
    "                    # Save average value and local values for each batch size\n",
    "\n",
    "                    self.__saveMeasure(pair_name, self.miHistory, self.miHistoryLocal, resultMI, local_resultsMI[-self.bs:])\n",
    "\n",
    "                if measures['tek']==True:\n",
    "                    # Calculate TE Using non binarized values (!!! in nats)\n",
    "\n",
    "                    calc_setup_fn = lambda : self.__setupTEKraskovCalculator()  \n",
    "                    resultTEK, local_resultsTEK = \\\n",
    "                        self.calculateMeasureBetweenNeurons(collector, neuron_i, neuron_j, self.past, calc_setup_fn, calc_name='kraskov', p_value=100)\n",
    "\n",
    "                    # Save average value and local values for each batch size\n",
    "\n",
    "                    self.__saveMeasure(pair_name, self.tekHistory, self.tekHistoryLocal, resultTEK, local_resultsTEK[-self.bs:])\n",
    "\n",
    "                if measures['mik']==True:\n",
    "                    # Calculate MI Using non binarized values (!!! in nats)\n",
    "\n",
    "                    calc_setup_fn = lambda : self.__setupMIKraskovCalculator()  \n",
    "                    resultMIK, local_resultsMIK = \\\n",
    "                        self.calculateMeasureBetweenNeurons(collector, neuron_i, neuron_j, self.past, calc_setup_fn, calc_name='kraskov', p_value=100)\n",
    "\n",
    "                    # Save average value and local values for each batch size\n",
    "\n",
    "                    self.__saveMeasure(pair_name, self.mikHistory, self.mikHistoryLocal, resultMIK, local_resultsMIK[-self.bs:])\n",
    "                \n",
    "    def calculate(self, collector, measures):\n",
    "        ''' Calculate Information Theory measures (discrete MI, TE or Kraskov MI, TE) \n",
    "        between neurons pairs\n",
    "        Note: the layer 0 is the input layer, i.e. it contains the input data\n",
    "         '''\n",
    "\n",
    "        num_layers=collector.num_layers\n",
    "\n",
    "        # for target_layer in range(1, num_layers):\n",
    "        #    self.getMeasuresForLayer(collector, target_layer-1, target_layer)\n",
    "\n",
    "        for source_layer in range(0, num_layers-1):\n",
    "            for target_layer in range(source_layer+1, num_layers):\n",
    "                self.getMeasuresForLayer(collector, source_layer, target_layer, measures)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class TeParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TeParameters:\n",
    "\n",
    "    \"\"\"Store the last calculated transfer entropies for each layer weights\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.te_params = self.__setupTEParameters(model)  #list of tensor same shape and order as model weights that will contain the corresponding te\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield self.te_params\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.te_params\n",
    "\n",
    "    def update(self, model, calculator):\n",
    "        '''Update the tensors containing the te values with the latest TE calculation value'''\n",
    "\n",
    "        layer = 1\n",
    "        for i, (name, parameter) in enumerate(model.named_parameters()):\n",
    "            # this makes it specifc to a Linear module (the parameters containing the weights are called 'weight')\n",
    "            if 'weight' in name:\n",
    "                # this makes it specific to a \"flat\" network (input = 1d) i think\n",
    "                for src_num_i in range(parameter.size()[1]-1):\n",
    "                    for target_num_j in range(parameter.size()[0]-1):\n",
    "                        neuron_i = (layer-1, src_num_i)\n",
    "                        neuron_j = (layer, target_num_j)\n",
    "                        # Note: we could update to calculate the value of te between neurons here instead of taking from history\n",
    "                        self.te_params[i][src_num_i][target_num_j] = calculator.getLastValueInHistory(calculator.teHistory, neuron_i, neuron_j)\n",
    "\n",
    "                #print(f'Updated TE for parameter {i} with name {name}: TE =  {self.te_params[i]}')\n",
    "                layer=layer+1\n",
    "            else:\n",
    "                self.te_params[i]=None\n",
    "\n",
    "    def __setupTEParameters(self, model):\n",
    "        ''' Initialize tensors of same size as model weights (not including bias) to contain the transfer entropy'''\n",
    "\n",
    "        te_params = []\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                te_param = torch.zeros_like(parameter, requires_grad=False)           \n",
    "                te_params.append(te_param)\n",
    "            else:\n",
    "                te_params.append(None)\n",
    "        return te_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0., 0.],\n",
      "        [0., 0.]]), None, tensor([[0., 0.]]), None]\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------\n",
    "# Test Snippet for TEParameters\n",
    "#---------------------------------------------------------\n",
    "\n",
    "\n",
    "model=XORnet()\n",
    "param = TeParameters(model)\n",
    "calc_params = {\n",
    "    'te': {\n",
    "        'source_history': 1,\n",
    "        'target_history': 1\n",
    "    }\n",
    "}\n",
    "param.update(model, teCalculator(model, 40, 40, calc_params))\n",
    "for p in param:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "http://mcneela.github.io/machine_learning/2019/09/03/Writing-Your-Own-Optimizers-In-Pytorch.html     \n",
    "https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD.step   \n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/_functional.py    \n",
    "\n",
    "\n",
    "## F.sgd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py and updated \n",
    "\n",
    "from torch import Tensor\n",
    "from typing import List, Optional\n",
    "\n",
    "def sgd2(params: List[Tensor],\n",
    "        d_p_list: List[Tensor],\n",
    "        momentum_buffer_list: List[Optional[Tensor]],\n",
    "        *,\n",
    "        weight_decay: float,\n",
    "        momentum: float,    \n",
    "        lr: float,\n",
    "        dampening: float,\n",
    "        nesterov: bool,\n",
    "        te_params: List[Tensor]):\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "\n",
    "        d_p = d_p_list[i]\n",
    "        if weight_decay != 0:\n",
    "            d_p = d_p.add(param, alpha=weight_decay)\n",
    "\n",
    "        if momentum != 0:\n",
    "            buf = momentum_buffer_list[i]\n",
    "\n",
    "            if buf is None:\n",
    "                buf = torch.clone(d_p).detach()\n",
    "                momentum_buffer_list[i] = buf\n",
    "            else:\n",
    "                buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "\n",
    "            if nesterov:\n",
    "                d_p = d_p.add(buf, alpha=momentum)\n",
    "            else:\n",
    "                d_p = buf\n",
    "\n",
    "        #print('param before descent:', param)\n",
    "        if te_params is not None:\n",
    "            te_param = te_params[i]\n",
    "            if te_param is not None:\n",
    "                #print('corresponding te_param:', te_param)\n",
    "                param.add_(d_p * (1 - te_param), alpha=-lr)\n",
    "            else:\n",
    "                param.add_(d_p, alpha=-lr)\n",
    "        else:     \n",
    "            param.add_(d_p, alpha=-lr)\n",
    "        #print('param after descent:', param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD2 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD.step\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim import _functional as F\n",
    "\n",
    "class SGD2(Optimizer):\n",
    "    '''\n",
    "    Implement backpropagation algorithm with a twist\n",
    "    Using inter-neuron transfer entropy to change the weight update\n",
    "    Wl:= Wl − η∆l(1 − tel)\n",
    "    '''\n",
    "    def __init__(self, params, te_params, lr=1e-3,momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False):\n",
    "\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "\n",
    "        defaults = dict(te_params = te_params, lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(SGD2, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(SGD2, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            d_p_list = []\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum_buffer_list = []\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "            lr = group['lr']\n",
    "\n",
    "            te_params=group['te_params']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    d_p_list.append(p.grad)\n",
    "\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'momentum_buffer' not in state:\n",
    "                        momentum_buffer_list.append(None)\n",
    "                    else:\n",
    "                        momentum_buffer_list.append(state['momentum_buffer'])\n",
    "\n",
    "            sgd2(params_with_grad,\n",
    "                d_p_list,\n",
    "                momentum_buffer_list,\n",
    "                weight_decay=weight_decay,\n",
    "                momentum=momentum,\n",
    "                lr=lr,\n",
    "                dampening=dampening,\n",
    "                nesterov=nesterov,\n",
    "                te_params=te_params)\n",
    "\n",
    "            # update momentum_buffers in state\n",
    "            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
    "                state = self.state[p]\n",
    "                state['momentum_buffer'] = momentum_buffer\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class learner():\n",
    "\n",
    "    def __init__(self, model, calculator, lr, opt_name, loss_fn=nn.MSELoss(), te_params=None):\n",
    "        self.model = model\n",
    "        self.calculator = calculator\n",
    "        self.lr = lr\n",
    "        self.loss_fn = loss_fn\n",
    "        print('learner lr:', lr)\n",
    "        \n",
    "        sgd2_te_params = None\n",
    "        if te_params is not None:\n",
    "            sgd2_te_params = te_params()\n",
    "        print('learner te_params:', sgd2_te_params)\n",
    "        optimizers = {\n",
    "            'adam': torch.optim.Adam(model.parameters(), lr = lr) ,\n",
    "            'SGD': torch.optim.SGD(model.parameters(), lr = lr), \n",
    "            'SGD2': SGD2(model.parameters(),sgd2_te_params , lr = lr)\n",
    "        }\n",
    "        self.opt = optimizers[opt_name]\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model, self.opt, self.loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot/save training stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTraining(loggers: list, learner: learner, run_name: str):\n",
    "    \n",
    "    fig, axs= plt.subplots(1, 2, figsize=(18, 6))  \n",
    "    plt.title('XOR Dataset Opt={}, lr={}, loss_fn={}'\n",
    "        .format(learner.opt.__class__.__name__, learner.lr, learner.loss_fn.__class__.__name__))\n",
    "\n",
    "    axs[0].plot(loggers['train'].losses, label='Train loss')\n",
    "    axs[0].plot(loggers['valid'].losses, label='Valid loss')\n",
    "    axs[0].set_title('Losses')\n",
    "    axs[0].set_xlabel('epochs')\n",
    "    axs[0].set_ylabel('loss')\n",
    "\n",
    "    axs[1].plot(loggers['train'].accuracies, label='Train accuracy')\n",
    "    axs[1].plot(loggers['valid'].accuracies, label='Valid accuracy')\n",
    "    axs[1].set_title('Accuracies')  \n",
    "    axs[1].set_xlabel('epochs')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "\n",
    "    plt.legend()\n",
    "    #plt.savefig(f'{run_name}-PlotEpoch.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot per minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingPerMiniBatch(loggers: list, learner: learner, run_name: str):\n",
    "    \n",
    "    fig, axs= plt.subplots(1, 2, figsize=(18, 6))  \n",
    "    plt.title('XOR Dataset Opt={}, lr={}, loss_fn={}'\n",
    "        .format(learner.opt.__class__.__name__, learner.lr, learner.loss_fn.__class__.__name__))\n",
    "\n",
    "    axs[0].plot(loggers['train'].all_minibatches_losses, label='Minibatch Train loss')\n",
    "    #axs[0].plot(loggers['valid'].all_minibatches_losses, label='Minibatch Valid loss')\n",
    "    axs[0].set_title('Losses')\n",
    "    axs[0].set_xlabel('Minibatches')\n",
    "    axs[0].set_ylabel('loss')\n",
    "\n",
    "    axs[1].plot(loggers['train'].all_minibatches_accuracies, label='Minibatch Train accuracy')\n",
    "    #axs[1].plot(loggers['valid'].all_minibatches_accuracies, label='Minibatch Valid accuracy')\n",
    "    axs[1].set_title('Accuracies')  \n",
    "    axs[1].set_xlabel('Minibatches')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "\n",
    "    plt.legend()\n",
    "    #plt.savefig(f'{run_name}-PlotMiniBatch.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(learner, loaders, epochs, collector, teCalc, teParams, target_accuracy, calculateITMeasures=True, measures={'te':True, 'mi':True, 'tek':True, 'mik': True}):\n",
    "\n",
    "    model, optimizer, loss_fn = learner.get_model()\n",
    "    trainLogger = statsLogger(learner)\n",
    "    validLogger = statsLogger(learner)\n",
    "\n",
    "    accuracy_reached=False\n",
    "    accuracy_kept=0\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "\n",
    "        trainLogger.startEpoch()\n",
    "\n",
    "        ## training part \n",
    "        ##--------------\n",
    "\n",
    "        model.train()\n",
    "        for i, (xb, yb) in enumerate(loaders['train']):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## 1. forward propagation\n",
    "            pred = model(xb)\n",
    "\n",
    "            if calculateITMeasures == True:\n",
    "                # collect activations\n",
    "                collector.collect(model, yb)\n",
    "                # calculate measures\n",
    "                teCalc.calculate(collector, measures)\n",
    "                \n",
    "                # TO DO: Only update for first epoch - Training Phase 1 - updating TEij to use for backprop\n",
    "                if epoch == 1:\n",
    "                    teParams.update(model, teCalc)\n",
    "            \n",
    "            ## 2. loss calculation\n",
    "            loss = loss_fn(pred, yb)        \n",
    "            \n",
    "            ## 3. backward propagation\n",
    "            loss.backward()\n",
    "            \n",
    "            ## 4. weight optimization\n",
    "            optimizer.step()\n",
    "\n",
    "            #logger\n",
    "            trainLogger.getMinibatchStats(pred, yb, loss)\n",
    "\n",
    "        epoch_accuracy = trainLogger.endEpoch()\n",
    "        #print(f'Epoch {epoch} completed with accuracy {epoch_accuracy}, target accuracy = {target_accuracy}')\n",
    "        if (epoch_accuracy == target_accuracy):\n",
    "            accuracy_reached=True\n",
    "            accuracy_kept+=1\n",
    "        if (epoch_accuracy != target_accuracy):\n",
    "            accuracy_reached=False\n",
    "            accuracy_kept=0\n",
    "\n",
    "        if (accuracy_kept == 5):\n",
    "            print(f'break epoch loop')\n",
    "            break\n",
    "            \n",
    "        ## evaluation part\n",
    "        ## ---------------\n",
    "\n",
    "        with torch.no_grad():  #gradients should not evaluate\n",
    "            model.eval()\n",
    "            validLogger.startEpoch()\n",
    "            for xb, yb in loaders['valid']:\n",
    "                pred = model(xb)\n",
    "                loss = loss_fn(pred, yb)\n",
    "\n",
    "                validLogger.getMinibatchStats(pred, yb, loss)\n",
    "\n",
    "            _ = validLogger.endEpoch()\n",
    "\n",
    "    return { 'train' : trainLogger, 'valid' : validLogger }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr=0.03, bs=40, num_samples=200, g=0.7, epochs=100, past=40, \\\n",
    "    opt_name='adam', calculator_params={'te':{'target_history':1, 'source_history':1}}, withTE=False, \\\n",
    "        calculateITMeasures=True, measures={'te':True, 'mi':True, 'tek':True, 'mik': True}):\n",
    "\n",
    "    model=XORnet()\n",
    "\n",
    "    _, loaders = get_data(bs, num_samples)\n",
    "\n",
    "    activation_collector=activationsCollector(g)\n",
    "    teCalc=teCalculator(model, past, bs, calculator_params)\n",
    "\n",
    "    teParams=TeParameters(model)\n",
    "    if withTE == True:                  # to update backpropgation algorithm using TE\n",
    "        optimizer_teParams=teParams\n",
    "        calculateITMeasures=True\n",
    "        measures['te']=True\n",
    "    else:\n",
    "        optimizer_teParams=None\n",
    "\n",
    "    Xorln = learner(model=model, calculator=teCalc, lr=lr, opt_name=opt_name, te_params=optimizer_teParams)\n",
    "\n",
    "    loggers = fit(Xorln, loaders, epochs, activation_collector, teCalc, teParams, target_accuracy=100, calculateITMeasures=calculateITMeasures, measures=measures)\n",
    "\n",
    "    lg=loggers['train']\n",
    "    print(f'{lg.epochs} epochs completed, epoch minibatches/samples: {lg.epoch_total_minibatches}/{lg.epoch_total_samples}, total minibs/samples:{lg.minibatches}/{lg.samples}')\n",
    "\n",
    "    run_name = lg.getRunName()\n",
    "\n",
    "    # Plot training\n",
    "\n",
    "    plotTraining(loggers, Xorln, run_name)\n",
    "    plotTrainingPerMiniBatch(loggers, Xorln, run_name)\n",
    "\n",
    "    # Saving data\n",
    "\n",
    "    print(f'Saving data for run {run_name}')\n",
    "    activation_collector.save(run_name)\n",
    "    teCalc.save(run_name)\n",
    "    lg.save('Train', 100)\n",
    "\n",
    "    return activation_collector, teCalc, lg\n",
    "\n",
    "# java.lang.RuntimeException: java.lang.RuntimeException: Base and history combination too large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learner lr: 0.03\n",
      "learner te_params: None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'teCalculator' object has no attribute 'calculate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32485/3962205486.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         }\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mactivation_collector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteCalc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len of accuracies:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_32485/576308881.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(lr, bs, num_samples, g, epochs, past, opt_name, calculator_params, withTE, calculateITMeasures, measures)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mXorln\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteCalc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_teParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mloggers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXorln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_collector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteCalc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculateITMeasures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalculateITMeasures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeasures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mlg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloggers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_32485/1614853923.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(learner, loaders, epochs, collector, teCalc, teParams, target_accuracy, calculateITMeasures, measures)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mcollector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;31m# calculate measures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mteCalc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# TO DO: Only update for first epoch - Training Phase 1 - updating TEij to use for backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'teCalculator' object has no attribute 'calculate'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "calculator_params = {\n",
    "    'te': {\n",
    "        'target_history': 1,\n",
    "        'source_history': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "#lrs = [ 0.03, 0.06, 0.12, 0.24 ]\n",
    "lrs= [ 0.03 ]\n",
    "for lr in lrs:\n",
    "\n",
    "    for i in range(1):\n",
    "\n",
    "        train_params = {\n",
    "            'lr' : lr, #0.03, 0.025 in paper\n",
    "            'bs' : 40,\n",
    "            'past' : 80,\n",
    "            'num_samples' : 200,\n",
    "            'g' : 0.7,\n",
    "            'epochs': 300, #100\n",
    "            'opt_name': 'adam',\n",
    "            'calculator_params': calculator_params,\n",
    "            'withTE': False,   # using TE in backpropagation, only used with optimizer SGD2\n",
    "            'calculateITMeasures': True,   # calculate IT Measures at each minibatch \n",
    "            'measures': {'te':False, 'mi':False, 'tek':True, 'mik': True}\n",
    "        }\n",
    "\n",
    "        activation_collector, teCalc, lg = train(**train_params)\n",
    "\n",
    "        print(\"len of accuracies:\", len(lg.accuracies))\n",
    "        epoch_with_target_accuracy_or_last_epoch = next((i+1 for i, acc in enumerate(lg.accuracies) if acc >= 100), len(lg.accuracies))\n",
    "\n",
    "        print(f'lr {lr} # epoch 100% acc or max {i} : acc {lg.accuracies[epoch_with_target_accuracy_or_last_epoch-1]} - losses {lg.losses[epoch_with_target_accuracy_or_last_epoch-1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_params_SGD2WithTE = {\n",
    "    'lr' : 0.03, #0.03, 0.025 in paper\n",
    "    'bs' : 40,\n",
    "    'past' : 40,\n",
    "    'num_samples' : 200,\n",
    "    'g' : 0.7,\n",
    "    'epochs': 300, #100\n",
    "    'opt_name': 'SGD2',\n",
    "    'calculator_params': calculator_params,\n",
    "    'withTE': True\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    activation_collector, teCalc, lg = train(**train_params_SGD2WithTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs= plt.subplots(1, 1, figsize=(18, 6))  \n",
    "plt.title('XORNet activations H1 H2 per target value')\n",
    "\n",
    "neurons = activation_collector.neurons\n",
    "\n",
    "t0_idx = [i for i in range(len(activation_collector.targets)) if activation_collector.targets[i] == 0]\n",
    "t1_idx = [i for i in range(len(activation_collector.targets)) if activation_collector.targets[i] == 1]\n",
    "i01_idx = [i for i in range(len(activation_collector.targets)) if activation_collector.neurons['0-0'][i] == 0 and activation_collector.neurons['0-1'][i] == 1 ]\n",
    "i10_idx = [i for i in range(len(activation_collector.targets)) if activation_collector.neurons['0-0'][i] == 1 and activation_collector.neurons['0-1'][i] == 0 ]\n",
    "i00_idx = [i for i in range(len(activation_collector.targets)) if activation_collector.neurons['0-0'][i] == 0 and activation_collector.neurons['0-1'][i] == 0 ]\n",
    "i11_idx = [i for i in range(len(activation_collector.targets)) if activation_collector.neurons['0-0'][i] == 1 and activation_collector.neurons['0-1'][i] == 1 ]\n",
    "\n",
    "axs.plot([ neurons['1-0'][i] for i in i01_idx ], linewidth=1, label='Hidden Neuron 1 - Input 0-1')\n",
    "axs.plot([ neurons['1-0'][i] for i in i10_idx ], linewidth=1, label='Hidden Neuron 1 - Input 1-0')\n",
    "axs.plot([ neurons['1-0'][i] for i in i00_idx ], linewidth=1, label='Hidden Neuron 1 - Input 0-0')\n",
    "axs.plot([ neurons['1-0'][i] for i in i11_idx ], linewidth=1, label='Hidden Neuron 1 - Input 1-1')\n",
    "# axs.plot([ activation_collector.neurons['1-0'][i] for i in t1_idx ],  linewidth=1, label='Hidden Neuron 1 - Target 1')\n",
    "\n",
    "axs.set_xlabel('activations')\n",
    "axs.set_ylabel('sample')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, axs= plt.subplots(1, 1, figsize=(18, 6))  \n",
    "axs.plot([ neurons['1-1'][i] for i in i01_idx ], linewidth=1, label='Hidden Neuron 2 - Input 0-1')\n",
    "axs.plot([ neurons['1-1'][i] for i in i10_idx ], linewidth=1, label='Hidden Neuron 2 - Input 1-0')\n",
    "axs.plot([ neurons['1-1'][i] for i in i00_idx ], linewidth=1, label='Hidden Neuron 2 - Input 0-0')\n",
    "axs.plot([ neurons['1-1'][i] for i in i11_idx ], linewidth=1, label='Hidden Neuron 2 - Input 1-1')\n",
    "\n",
    "axs.set_xlabel('activations')\n",
    "axs.set_ylabel('sample')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, axs= plt.subplots(1, 1, figsize=(18, 6))  \n",
    "axs.plot([ neurons['2-0'][i] for i in i01_idx ], linewidth=1, label='Output Neuron - Input 0-1')\n",
    "axs.plot([ neurons['2-0'][i] for i in i10_idx ], linewidth=1, label='Output Neuron - Input 1-0')\n",
    "axs.plot([ neurons['2-0'][i] for i in i00_idx ], linewidth=1, label='Output Neuron - Input 0-0')\n",
    "axs.plot([ neurons['2-0'][i] for i in i11_idx ], linewidth=1, label='Output Neuron - Input 1-1')\n",
    "\n",
    "axs.set_xlabel('activations')\n",
    "axs.set_ylabel('sample')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "normalizl = lambda lst: scaler.fit_transform(np.array(lst).reshape(len(lst), 1))\n",
    "\n",
    "fig, axs= plt.subplots(1, 1, figsize=(18, 6))  \n",
    "plt.title('Transfer entropies between each pair of neurons')\n",
    "axs.set_xlabel('training time')\n",
    "axs.set_ylabel('TE')\n",
    "\n",
    "\n",
    "\n",
    "history = pd.DataFrame(teCalc.tekHistory)/np.log(2)  # kraskov te in bits\n",
    "historymi = teCalc.miHistory\n",
    "\n",
    "axs.plot(history['0-0-1-0'], linewidth=1, label='TE between neuron 0-0 and 1-0: TE_1_00')\n",
    "axs.plot(historymi['0-0-1-0'], linewidth=1, label='MI between neuron 0-0 and 1-0: MI_1_00')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, axs= plt.subplots(1, 1, figsize=(18, 6))  \n",
    "plt.title('Transfer entropies between each pair of neurons')\n",
    "axs.set_xlabel('training time')\n",
    "axs.set_ylabel('TE')\n",
    "\n",
    "axs.plot(history['0-0-1-1'], linewidth=1, label='TE between neuron 0-0 and 1-1: TE_1_01')\n",
    "axs.plot(historymi['0-0-1-1'], linewidth=1, label='MI between neuron 0-0 and 1-1: MI_1_01')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, axs= plt.subplots(1, 1, figsize=(18, 6))  \n",
    "plt.title('Transfer entropies between each pair of neurons')\n",
    "axs.set_xlabel('training time')\n",
    "axs.set_ylabel('TE')\n",
    "\n",
    "axs.plot(history['0-1-1-0'], linewidth=1, label='TE between neuron 0-1 and 1-0: TE_1_10')\n",
    "axs.plot(historymi['0-1-1-0'], linewidth=1, label='MI between neuron 0-1 and 1-0: MI_1_10')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, axs= plt.subplots(1, 1, figsize=(18, 6))  \n",
    "plt.title('Transfer entropies between each pair of neurons')\n",
    "axs.set_xlabel('training time')\n",
    "axs.set_ylabel('TE')\n",
    "\n",
    "axs.plot(history['0-1-1-1'], linewidth=1, label='TE between neuron 0-1 and 1-1: TE_1_11')\n",
    "axs.plot(historymi['0-1-1-1'], linewidth=1, label='MI between neuron 0-1 and 1-1: MI_1_11')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axs= plt.subplots(1, 1, figsize=(18, 6))  \n",
    "plt.title('Transfer entropies between each pair of neurons')\n",
    "axs.set_xlabel('training time')\n",
    "axs.set_ylabel('TE')\n",
    "\n",
    "axs.plot(history['1-0-2-0'], linewidth=1, label='TE between neuron 1-0 and 2-0: TE_2_00')\n",
    "axs.plot(historymi['1-0-2-0'], linewidth=1, label='MI between neuron 1-0 and 2-0: MI_2_00')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, axs= plt.subplots(1, 1, figsize=(18, 6))  \n",
    "plt.title('Transfer entropies between each pair of neurons')\n",
    "axs.set_xlabel('training time')\n",
    "axs.set_ylabel('bits')\n",
    "\n",
    "axs.plot(history['1-1-2-0'], linewidth=1, label='TE between neuron 1-1 and 2-0: TE_2_10')\n",
    "axs.plot(historymi['1-1-2-0'], linewidth=1, label='MI between neuron 1-1 and 2-0: MI_2_10')\n",
    "#axs.plot(normalizl(loggers['train'].all_minibatches_accuracies), label='Minibatch Train accuracy')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(teCalc.tekHistory)\n",
    "history2 = pd.DataFrame(teCalc.tekHistory) * 2\n",
    "history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs= plt.subplots(1, 1, figsize=(18, 6))  \n",
    "plt.title('Transfer entropies between each pair of neurons')\n",
    "\n",
    "t0_idx = [i for i in range(len(activation_collector.targets)) if activation_collector.targets[i] == 0]\n",
    "t1_idx = [i for i in range(len(activation_collector.targets)) if activation_collector.targets[i] == 1]\n",
    "i01_idx = [i for i in range(len(activation_collector.targets)) if activation_collector.neurons['0-0'][i] == 0 and activation_collector.neurons['0-1'][i] == 1 ]\n",
    "i10_idx = [i for i in range(len(activation_collector.targets)) if activation_collector.neurons['0-0'][i] == 1 and activation_collector.neurons['0-1'][i] == 0 ]\n",
    "i00_idx = [i for i in range(len(activation_collector.targets)) if activation_collector.neurons['0-0'][i] == 0 and activation_collector.neurons['0-1'][i] == 0 ]\n",
    "i11_idx = [i for i in range(len(activation_collector.targets)) if activation_collector.neurons['0-0'][i] == 1 and activation_collector.neurons['0-1'][i] == 1 ]\n",
    "\n",
    "axs.plot([ activation_collector.neurons['1-0'][i] for i in i01_idx ], linewidth=1, label='Hidden Neuron 1 - Input 0-1')\n",
    "axs.plot([ activation_collector.neurons['1-0'][i] for i in i10_idx ], linewidth=1, label='Hidden Neuron 1 - Input 1-0')\n",
    "axs.plot([ activation_collector.neurons['1-0'][i] for i in i00_idx ], linewidth=1, label='Hidden Neuron 1 - Input 0-0')\n",
    "axs.plot([ activation_collector.neurons['1-0'][i] for i in i11_idx ], linewidth=1, label='Hidden Neuron 1 - Input 1-1')\n",
    "# axs.plot([ activation_collector.neurons['1-0'][i] for i in t1_idx ],  linewidth=1, label='Hidden Neuron 1 - Target 1')\n",
    "\n",
    "axs.set_xlabel('activations')\n",
    "axs.set_ylabel('sample')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, axs= plt.subplots(1, 1, figsize=(18, 6))  \n",
    "axs.plot([ activation_collector.neurons['1-1'][i] for i in i01_idx ], linewidth=1, label='Hidden Neuron 2 - Input 0-1')\n",
    "axs.plot([ activation_collector.neurons['1-1'][i] for i in i10_idx ], linewidth=1, label='Hidden Neuron 2 - Input 1-0')\n",
    "axs.plot([ activation_collector.neurons['1-1'][i] for i in i00_idx ], linewidth=1, label='Hidden Neuron 2 - Input 0-0')\n",
    "axs.plot([ activation_collector.neurons['1-1'][i] for i in i11_idx ], linewidth=1, label='Hidden Neuron 2 - Input 1-1')\n",
    "\n",
    "axs.set_xlabel('activations')\n",
    "axs.set_ylabel('sample')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, axs= plt.subplots(1, 1, figsize=(18, 6))  \n",
    "axs.plot([ activation_collector.neurons['2-0'][i] for i in i01_idx ], linewidth=1, label='Output Neuron - Input 0-1')\n",
    "axs.plot([ activation_collector.neurons['2-0'][i] for i in i10_idx ], linewidth=1, label='Output Neuron - Input 1-0')\n",
    "axs.plot([ activation_collector.neurons['2-0'][i] for i in i00_idx ], linewidth=1, label='Output Neuron - Input 0-0')\n",
    "axs.plot([ activation_collector.neurons['2-0'][i] for i in i11_idx ], linewidth=1, label='Output Neuron - Input 1-1')\n",
    "\n",
    "axs.set_xlabel('activations')\n",
    "axs.set_ylabel('sample')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs= plt.subplots(1, 1, figsize=(18, 6))  \n",
    "plt.title('XORNet activations H1 H2')\n",
    "\n",
    "\n",
    "axs.plot(activation_collector.neurons['1-0'], linestyle=\":\", linewidth=1, label='Hidden Neuron 1')\n",
    "\n",
    "axs.set_xlabel('activations')\n",
    "axs.set_ylabel('sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = torch.tensor([[1.0, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc1.weight, model.fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc2.weight, model.fc2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print('------------------------------------------')\n",
    "    print(param)\n",
    "    print(type(param))\n",
    "    print(type(param.data), param.size())\n",
    "    print(param.data)\n",
    "    print(param.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(name, parameter)\n",
    "        print(parameter.size()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizer\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
