{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# torch, autograd and backpropagation\n",
    "# https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/\n",
    "\n",
    "\n",
    "import torch \n",
    "\n",
    "a = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "w1 = torch.randn((3,3), requires_grad = True)\n",
    "w2 = torch.randn((3,3), requires_grad = True)\n",
    "w3 = torch.randn((3,3), requires_grad = True)\n",
    "w4 = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "b = w1*a \n",
    "c = w2*a\n",
    "\n",
    "d = w3*b + w4*c \n",
    "\n",
    "L = 10 - d\n",
    "\n",
    "print(\"The grad fn for a is\", a.grad_fn)\n",
    "print(\"The grad fn for d is\", d.grad_fn)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The grad fn for a is None\n",
      "The grad fn for d is <AddBackward0 object at 0x7f9df81cdb20>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "b.is_leaf"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "w1.is_leaf"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "b.grad_fn"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<MulBackward0 at 0x7f9e00683d60>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import torch \n",
    "a = torch.ones(5)\n",
    "a.requires_grad = True\n",
    "\n",
    "b = 2*a\n",
    "\n",
    "b.retain_grad()   # Since b is non-leaf and it's grad will be destroyed otherwise.\n",
    "\n",
    "c = b.mean()\n",
    "\n",
    "c.backward()\n",
    "\n",
    "print(a.grad, b.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Redo the experiment but with a hook that multiplies b's grad by 2. \n",
    "a = torch.ones(5)\n",
    "\n",
    "a.requires_grad = True\n",
    "\n",
    "b = 2*a\n",
    "\n",
    "b.retain_grad()\n",
    "\n",
    "b.register_hook(lambda x: print('Hello Im a tensor backward hook reporting on x:', x))  \n",
    "\n",
    "b.mean().backward() \n",
    "\n",
    "\n",
    "print(a.grad, b.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello Im a tensor backward hook reporting on x: tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Playing with hooks (https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/)\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class myNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Conv2d(3,10,2, stride = 2)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.flatten = lambda x: x.view(-1)\n",
    "    self.fc1 = nn.Linear(160,5)\n",
    "   \n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.relu(self.conv(x))\n",
    "    x.register_hook(lambda grad : torch.clamp(grad, min = 0))     #No gradient shall be backpropagated \n",
    "                                                                  #conv outside less than 0\n",
    "      \n",
    "    # print whether there is any negative grad\n",
    "    x.register_hook(lambda grad: print(\"Gradients less than zero:\", bool((grad < 0).any())))  \n",
    "    return self.fc1(self.flatten(x))\n",
    "  \n",
    "\n",
    "net = myNet()\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "  # if the param is from a linear and is a bias\n",
    "  if \"fc\" in name and \"bias\" in name:\n",
    "    param.register_hook(lambda grad: torch.zeros(grad.shape))\n",
    "\n",
    "\n",
    "out = net(torch.randn(1,3,8,8)) \n",
    "\n",
    "(1 - out).mean().backward()\n",
    "\n",
    "print(\"The biases are\", net.fc1.bias.grad)     #bias grads are zero"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradients less than zero: False\n",
      "The biases are tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# The Forward Hook for Visualising Activations \n",
    "\n",
    "visualisation = {}\n",
    "\n",
    "inp = torch.randn(1,3,8,8)\n",
    "\n",
    "def hook_fn(m, i, o):\n",
    "  visualisation[m] = o \n",
    "  \n",
    "net = myNet()\n",
    "\n",
    "for name, layer in net._modules.items():\n",
    "  print('net._modules.items().__class__.__name__:', net._modules.items())\n",
    "  layer.register_forward_hook(hook_fn)\n",
    "  \n",
    "out = net(inp) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[ 1.7005, -1.9498, -0.4868, -0.0303, -0.3445,  0.2933, -0.8732,\n",
      "           -0.6807],\n",
      "          [-2.0932, -0.9393, -0.1748,  0.4936,  1.1228, -0.6647, -1.7640,\n",
      "           -1.0301],\n",
      "          [ 0.2753,  1.0046,  0.5040, -0.7411,  0.1099, -0.0484, -0.4029,\n",
      "            2.0737],\n",
      "          [-1.4201, -0.7080,  0.0212, -1.2915, -0.4933,  1.1316, -0.9295,\n",
      "            0.5645],\n",
      "          [-1.9732,  0.2441, -0.3288, -0.7064,  0.5568,  0.7822,  0.1295,\n",
      "            0.8211],\n",
      "          [-0.7719,  0.7438, -0.2226,  0.0795,  0.3837,  0.1072, -0.7230,\n",
      "            0.2285],\n",
      "          [-1.1002, -0.1511,  0.4822,  0.7264,  0.0208,  1.1334,  1.2841,\n",
      "            1.5398],\n",
      "          [ 0.4773, -0.1455,  0.7014,  1.7694,  1.2766, -0.1334, -2.1647,\n",
      "           -0.6898]],\n",
      "\n",
      "         [[-0.1906,  1.5169, -0.2449, -1.3430,  2.3499,  0.7428, -0.0139,\n",
      "           -1.3244],\n",
      "          [ 0.0750,  1.3354, -0.3667,  1.1475, -0.8353, -1.9871,  1.2343,\n",
      "            1.0611],\n",
      "          [ 0.0372, -0.4763,  1.8792, -0.1377, -0.9170, -0.5843, -1.9749,\n",
      "           -0.5249],\n",
      "          [ 1.0224,  2.4056, -0.2640,  0.8842,  0.1533, -0.0552, -0.1717,\n",
      "           -1.1263],\n",
      "          [-0.4655, -0.1636,  1.5527, -0.4819, -0.9189, -0.5429,  0.9222,\n",
      "           -2.2828],\n",
      "          [ 1.3118,  0.0111, -0.2633,  1.2584,  0.1307,  2.1443, -0.8316,\n",
      "            0.2845],\n",
      "          [ 1.0214, -0.6056, -0.7382,  0.0486, -0.2238,  0.6002, -0.1044,\n",
      "           -0.4211],\n",
      "          [ 0.1280, -1.7357,  0.2351,  0.5053, -0.6905, -1.2539,  1.3015,\n",
      "           -2.2418]],\n",
      "\n",
      "         [[ 1.3442, -0.4951,  2.5061, -0.0278,  1.0827,  0.1458, -0.3990,\n",
      "            0.1332],\n",
      "          [-1.3362, -1.3068,  0.2638,  0.1451, -1.2915,  2.2633, -1.6139,\n",
      "           -1.1435],\n",
      "          [-2.1175,  1.4958, -1.5845,  0.1233,  0.2673, -1.0498,  0.0903,\n",
      "           -0.8648],\n",
      "          [ 2.1707, -1.1155, -0.5350, -0.2093,  1.0928, -0.6963,  1.2453,\n",
      "            1.5645],\n",
      "          [-1.9042,  0.1436,  0.1881, -0.0388, -0.1076, -0.4715,  0.7145,\n",
      "            0.3455],\n",
      "          [-0.1483,  0.4428,  0.1255, -0.2305,  1.2377, -0.3094,  2.8353,\n",
      "           -0.5065],\n",
      "          [-0.9161,  0.9999, -1.2768,  0.7960,  0.9321, -0.9066, -1.5364,\n",
      "           -2.0665],\n",
      "          [-0.1115, -0.2126,  0.7183, -0.0274,  0.2386,  0.5377,  1.2926,\n",
      "            2.3593]]]])\n",
      "net._modules.items().__class__.__name__: odict_items\n",
      "net._modules.items().__class__.__name__: odict_items\n",
      "net._modules.items().__class__.__name__: odict_items\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}